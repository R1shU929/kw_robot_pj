// src/components/FaceCamera.jsx
import { useEffect, useRef, useState } from 'react'
import * as faceapi from 'face-api.js'

const MODEL_URL = '/models'

// public/faces Ìè¥ÎçîÏùò ÏÇ¨Îûå Ïù¥Î¶ÑÎì§
const LABELS = ['jumi', 'cho', 'yun', 'bae']

function FaceCamera({ onRecognized }) {
  const videoRef = useRef(null)
  const canvasRef = useRef(null)
  const [status, setStatus] = useState('Î™®Îç∏ Î°úÎî© Ï§ë...')

  // ‚≠ê Ïû¨Ïù∏ÏãùÏùÑ ÏúÑÌïú throttle ÏãúÍ∞Ñ Ï†ÄÏû•
  const lastRecognizedTimeRef = useRef(0)

  useEffect(() => {
    let stream
    let intervalId

    async function loadModels() {
      setStatus('Î™®Îç∏ Î°úÎî© Ï§ë...')
      await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL)
      await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL)
      await faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL)
      setStatus('Î™®Îç∏ Î°úÎî© ÏôÑÎ£å, Ïπ¥Î©îÎùº Ï§ÄÎπÑ Ï§ë...')
    }

    async function loadLabeledImages() {
      const labeledDescriptors = []

      for (const label of LABELS) {
        const descriptors = []

        for (let i = 1; i <= 5; i++) {
          const imgUrl = `/faces/${label}/${i}.jpg`
          try {
            const img = await faceapi.fetchImage(imgUrl)

            const detection = await faceapi
              .detectSingleFace(img, new faceapi.TinyFaceDetectorOptions())
              .withFaceLandmarks()
              .withFaceDescriptor()

            if (detection && detection.descriptor) {
              descriptors.push(detection.descriptor)
            }
          } catch (e) {
            console.warn('failed to load face image', imgUrl, e)
          }
        }

        if (descriptors.length > 0) {
          labeledDescriptors.push(
            new faceapi.LabeledFaceDescriptors(label, descriptors)
          )
        }
      }

      return labeledDescriptors
    }

    async function start() {
      try {
        await loadModels()
        const labeledDescriptors = await loadLabeledImages()
        const faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, 0.6)

        // ‚≠ê Ïπ¥Î©îÎùº Ïã§Ìñâ
        stream = await navigator.mediaDevices.getUserMedia({
          video: { facingMode: 'user' },
          audio: false,
        })

        if (!videoRef.current) return
        videoRef.current.srcObject = stream

        videoRef.current.onloadedmetadata = () => {
          const video = videoRef.current
          if (!video) return
          video.play()

          setStatus('Ïπ¥Î©îÎùº ÏºúÏßê ‚Äì ÏñºÍµ¥ÏùÑ ÌôîÎ©¥Ïóê ÎßûÏ∂∞Ï£ºÏÑ∏Ïöî.')

          const canvas = canvasRef.current
          const displaySize = {
            width: video.videoWidth || 640,
            height: video.videoHeight || 480,
          }

          canvas.width = displaySize.width
          canvas.height = displaySize.height

          intervalId = setInterval(async () => {
            if (!video || video.readyState !== 4) return

            const detections = await faceapi
              .detectAllFaces(
                video,
                new faceapi.TinyFaceDetectorOptions({ inputSize: 416 })
              )
              .withFaceLandmarks()
              .withFaceDescriptors()

            const resizedDetections = faceapi.resizeResults(
              detections,
              displaySize
            )

            const ctx = canvas.getContext('2d')
            ctx.clearRect(0, 0, canvas.width, canvas.height)

            faceapi.draw.drawDetections(canvas, resizedDetections)

            resizedDetections.forEach((d) => {
              const bestMatch = faceMatcher.findBestMatch(d.descriptor)

              const box = d.detection.box
              const drawBox = new faceapi.draw.DrawBox(box, {
                label: bestMatch.toString(),
              })
              drawBox.draw(canvas)

              // ==========================================================
              // ‚≠ê Î≥ÄÍ≤ΩÎêú Ïû¨Ïù∏Ïãù Î∞©Ïãù: 1.5Ï¥à ÎßàÎã§ Ïû¨Ïù∏Ïãù ÌóàÏö©
              // ‚≠ê Í∑∏Î¶¨Í≥† labelÏóêÏÑú Í±∞Î¶¨Í∞í Ï†úÍ±∞ (Ï∂úÏÑù Ïò§Î•ò Ìï¥Í≤∞ ÌïµÏã¨)
              // ==========================================================
              if (bestMatch.label !== 'unknown') {
                const now = Date.now()

                if (now - lastRecognizedTimeRef.current > 1500) {
                  lastRecognizedTimeRef.current = now

                  // üéØ label Ï†ïÏ†ú: "yun (0.43)" ‚Üí "yun"
                  const pureLabel = bestMatch.label.split(' ')[0]

                  onRecognized?.(pureLabel)
                  setStatus(`ÏñºÍµ¥ Ïù∏ÏãùÎê®: ${pureLabel}`)
                }
              }
            })
          }, 500)
        }
      } catch (err) {
        console.error(err)
        setStatus(`ÏóêÎü¨ Î∞úÏÉù: ${err.message}`)
      }
    }

    start()

    // cleanup
    return () => {
      if (intervalId) clearInterval(intervalId)
      if (stream) {
        stream.getTracks().forEach((track) => track.stop())
      }
    }
  }, [onRecognized])

  return (
    <div
      style={{
        position: 'relative',
        width: '100%',
        height: '100%',
        background: '#000',
        borderRadius: '12px',
        overflow: 'hidden',
      }}
    >
      <video
        ref={videoRef}
        autoPlay
        muted
        playsInline
        style={{ width: '100%', height: '100%', objectFit: 'cover' }}
      />
      <canvas
        ref={canvasRef}
        style={{
          position: 'absolute',
          top: 0,
          left: 0,
          width: '100%',
          height: '100%',
        }}
      />
      <div
        style={{
          position: 'absolute',
          top: 10,
          left: 10,
          color: 'white',
          fontSize: '0.9rem',
          textShadow: '0 0 4px rgba(0,0,0,0.8)',
          background: 'rgba(0,0,0,0.4)',
          padding: '4px 8px',
          borderRadius: 4,
        }}
      >
        {status}
      </div>
    </div>
  )
}

export default FaceCamera
